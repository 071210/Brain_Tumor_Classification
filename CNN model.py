# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-nsg3qnYCHyiOJqqLKPW7zO9KN1ZXp4u
"""

# General Imports
import tensorflow as tf
import pandas as pd
import numpy as np
import random
import os

# Visualization
import matplotlib.pyplot as plt

# Building Model
from tensorflow.keras import models
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten

# Training Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import ModelCheckpoint

# Data Processing
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import load_img

import kagglehub

# Download latest version
path = kagglehub.dataset_download("masoudnickparvar/brain-tumor-mri-dataset")

print("Path to dataset files:", path)

# Commented out IPython magic to ensure Python compatibility.
# Global variables
SEED = 111

# Setting seed for consistent results
tf.keras.utils.set_random_seed(SEED)
tf.random.set_seed(SEED)
np.random.seed(SEED)

# Data Visualization updates
# %config InlineBackend.figure_format = 'retina'
plt.rcParams["figure.figsize"] = (16, 10)
plt.rcParams.update({'font.size': 14})

# Data Classifications
CLASS_TYPES = ['pituitary', 'notumor', 'meningioma', 'glioma']
N_TYPES = len(CLASS_TYPES)

import os

# Function to get image paths and labels
def get_data_labels(data_dir):
    image_paths = []
    labels = []
    for label_type in os.listdir(data_dir):
        label_dir = os.path.join(data_dir, label_type)
        if os.path.isdir(label_dir):
            for image_file in os.listdir(label_dir):
                if image_file.endswith(('.jpg', '.jpeg', '.png')):
                    image_paths.append(os.path.join(label_dir, image_file))
                    labels.append(label_type)
    return image_paths, labels

# Setting up file paths for training and testing
USER_PATH = path
train_dir = os.path.join(USER_PATH, "Training")
test_dir = os.path.join(USER_PATH, "Testing")

# Getting data using above function
train_paths, train_labels = get_data_labels(train_dir)
test_paths, test_labels = get_data_labels(test_dir)

# Printing traing and testing sample sizes
print('Training')
print(f'Number of Paths: {len(train_paths)}')
print(f'Number of Labels: {len(train_labels)}')
print('\nTesting')
print(f'Number of Paths: {len(test_paths)}')
print(f'Number of Labels: {len(test_labels)}')

def show_images_per_class(image_paths, labels, im_size=150, figsize=(16,10)):
    plt.figure(figsize=figsize)
    classes = list(set(labels))

    for i, cls in enumerate(classes):
        # get all indices of this class
        idx = [j for j, x in enumerate(labels) if x == cls]
        sample = random.choice(idx)

        plt.subplot(2, 2, i+1)
        img = load_img(image_paths[ sample ], target_size=(im_size, im_size))
        plt.imshow(img)
        plt.title(f"Label: {cls}")
        plt.axis("off")

    plt.tight_layout()
    plt.show()

show_images_per_class(train_paths, train_labels, im_size=350)

# Image size
image_size = (150, 150)

# Training batch size
batch_size = 32

# Data augmentation and preprocessing
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=10,
                                   brightness_range=(0.85, 1.15),
                                   width_shift_range=0.002,
                                   height_shift_range=0.002,
                                   shear_range=12.5,
                                   zoom_range=0,
                                   horizontal_flip=True,
                                   vertical_flip=False,
                                   fill_mode="nearest")


# applying the generator to training data with constant seed
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=image_size,
                                                    batch_size=batch_size,
                                                    class_mode="categorical",
                                                    seed=SEED)

# No augmentation of the test data, just rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# applying the generator to testing data with constant seed
test_generator = test_datagen.flow_from_directory(test_dir,
                                                  target_size=image_size,
                                                  batch_size=batch_size,
                                                  class_mode="categorical",
                                                  shuffle=False,
                                                  seed=SEED)

# Accessing class indices for training data generator
class_indices_train = train_generator.class_indices
class_indices_train_list = list(train_generator.class_indices.keys())


# Displaying categorical types
print("Categorical types for the training data:")
print(class_indices_train)

def show_ImageDataGenerator(image_datagen_obj, num_samples, figsize, save):
    plt.figure(figsize=figsize)
    # Create a temporary generator to get augmented images
    # using the provided ImageDataGenerator object and training directory
    temp_generator = image_datagen_obj.flow_from_directory(
        train_dir,
        target_size=image_size,
        batch_size=num_samples, # Get 'num_samples' images in one batch
        class_mode="categorical",
        shuffle=True, # Shuffle to get different images each time
        seed=SEED
    )

    # Get one batch of images and labels
    images, labels = next(temp_generator)

    # Map one-hot encoded labels back to class names
    label_map = {v: k for k, v in class_indices_train.items()}

    # Calculate rows and columns for subplot dynamically
    ncols = 4
    nrows = (num_samples + ncols - 1) // ncols

    for i in range(num_samples):
        plt.subplot(nrows, ncols, i + 1)
        plt.imshow(images[i])
        class_name = label_map[np.argmax(labels[i])]
        plt.title(f'Class: {class_name}')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

SAVE = False

show_ImageDataGenerator(train_datagen, num_samples=5, figsize=(12.5, 8), save=SAVE)

# Image shape: height, width, RBG
image_shape = (image_size[0], image_size[1], 3)

# Training epochs
epochs = 40

# Steps per epoch
steps_per_epoch = train_generator.samples // batch_size

# Validation steps
validation_steps = test_generator.samples // batch_size

print(f'Image shape: {image_shape}')
print(f'Epochs: {epochs}')
print(f'Batch size: {batch_size}')
print(f'Steps Per Epoch: {steps_per_epoch}')
print(f'Validation steps: {validation_steps}')

# Define the model architecture
model = models.Sequential([

    # Convolutional layer 1
    Conv2D(32, (4, 4), activation="relu", input_shape=image_shape),
    MaxPooling2D(pool_size=(3, 3)),

    # Convolutional layer 2
    Conv2D(64, (4, 4), activation="relu"),
    MaxPooling2D(pool_size=(3, 3)),

    # Convolutional layer 3
    Conv2D(128, (4, 4), activation="relu"),
    MaxPooling2D(pool_size=(3, 3)),

    # Convolutional layer 4
    Conv2D(128, (4, 4), activation="relu"),
    Flatten(),

    # Full connect layers
    Dense(512, activation="relu"),
    Dropout(0.5, seed=SEED),
    Dense(N_TYPES, activation="softmax")
])

model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.869, beta_2=0.995)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= ['accuracy'])

# Stop training if loss doesn't keep decreasing.
model_es = EarlyStopping(monitor='loss', min_delta=1e-9, patience=8, verbose=True)
model_rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, verbose=True)

# Training the model
history = model.fit(train_generator,
                    steps_per_epoch=steps_per_epoch,
                    epochs=epochs,
                    validation_data=test_generator,
                    validation_steps=validation_steps,
                    callbacks=[model_es, model_rlr])

model.save('brain-tumor-model.h5')

# Evaluating the model
loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples//batch_size)
print(f"Test Loss: {loss:0.5f}")
print(f"Test Accuracy: {accuracy:0.5f}")

_, ax = plt.subplots(ncols=2, figsize=(15, 6))

# Plot the training and validation accuracy over epochs
ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Model 2 Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].set_ylabel('Accuracy')
ax[0].legend(['Train', 'Validation'])
ax[0].grid(alpha=0.2)

# Plot the training and validation loss over epochs
ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Model 2 Loss')
ax[1].set_xlabel('Epoch')
ax[1].set_ylabel('Loss')
ax[1].legend(['Train', 'Validation'])
ax[1].grid(alpha=0.2)

plt.show()

from sklearn.metrics import confusion_matrix

def CM(CNN_model, test_generator, categories):
    y_pred = CNN_model.predict(test_generator)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = test_generator.classes
    cm = confusion_matrix(y_true, y_pred_classes)
    return cm

import seaborn as sns

# Plotting confusion matrix
confusion_matrix = CM(CNN_model=model, test_generator=test_generator, categories=class_indices_train_list)

plt.figure(figsize=(8,8))
sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.xticks(ticks=np.arange(N_TYPES) + 0.5,
           labels=[name.title() for name in class_indices_train_list], ha='center')
plt.yticks(ticks=np.arange(N_TYPES) + 0.5,
           labels=[name.title() for name in class_indices_train_list], va='center')
plt.show()

from sklearn.metrics import classification_report, accuracy_score

def calculate_metrics(cm, categories):
    # Calculate overall accuracy
    total_samples = np.sum(cm)
    correct_predictions = np.trace(cm)
    overall_accuracy = correct_predictions / total_samples
    print(f'Overall Accuracy: {overall_accuracy:.4f}\n')

    print('Classification Report:\n')
    report_dict = {}
    for i, class_name in enumerate(categories):
        true_positives = cm[i, i]
        false_positives = np.sum(cm[:, i]) - true_positives
        false_negatives = np.sum(cm[i, :]) - true_positives

        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        report_dict[class_name] = {
            'precision': precision,
            'recall': recall,
            'f1-score': f1_score,
            'support': np.sum(cm[i, :])
        }

    # Print the report in a formatted way
    print(f"{'':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print(f"{'':-<60}")
    for class_name, metrics in report_dict.items():
        print(f"{class_name.title():<15} {metrics['precision']:.4f}{'':<6} {metrics['recall']:.4f}{'':<6} {metrics['f1-score']:.4f}{'':<6} {metrics['support']:<10}")

    # Macro and Weighted Averages
    macro_precision = np.mean([m['precision'] for m in report_dict.values()])
    macro_recall = np.mean([m['recall'] for m in report_dict.values()])
    macro_f1 = np.mean([m['f1-score'] for m in report_dict.values()])

    total_support = np.sum([m['support'] for m in report_dict.values()])
    weighted_precision = np.sum([m['precision'] * m['support'] for m in report_dict.values()]) / total_support
    weighted_recall = np.sum([m['recall'] * m['support'] for m in report_dict.values()]) / total_support
    weighted_f1 = np.sum([m['f1-score'] * m['support'] for m in report_dict.values()]) / total_support

    print(f"{'':-<60}")
    print(f"{'Macro Avg':<15} {macro_precision:.4f}{'':<6} {macro_recall:.4f}{'':<6} {macro_f1:.4f}{'':<6} {total_support:<10}")
    print(f"{'Weighted Avg':<15} {weighted_precision:.4f}{'':<6} {weighted_recall:.4f}{'':<6} {weighted_f1:.4f}{'':<6} {total_support:<10}")

# Showing metrics
calculate_metrics(confusion_matrix, categories=class_indices_train_list)

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

def predict_class_and_probability(model, img_path, target_size=(150,150), class_indices=None):
    # Load and preprocess image
    img = load_img(img_path, target_size=target_size)
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # Predict
    prediction = model.predict(img_array)
    prob = np.max(prediction)
    pred_index = np.argmax(prediction)


    if class_indices:
        class_list = list(class_indices.keys())
        predicted_class = class_list[pred_index]
    else:
        predicted_class = pred_index

    return predicted_class, float(prob)

img_path = "/root/.cache/kagglehub/datasets/masoudnickparvar/brain-tumor-mri-dataset/versions/1/Testing/pituitary/Te-piTr_0000.jpg"

pred_class, prob = predict_class_and_probability(
    model,
    img_path,
    target_size=(150,150),
    class_indices=train_generator.class_indices
)

print("Predicted Class:", pred_class)
print("Probability:", prob)